{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Arun Das\n",
    "    Research Fellow\n",
    "    Secure AI and Autonomy Laboratory\n",
    "    University of Texas at San Antonio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'Capsule' invariance problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A capsule is a group of neurons whose activity vector represents the instantiation\n",
    "parameters of a specific type of entity such as an object or an object part. We use\n",
    "the length of the activity vector to represent the probability that the entity exists and\n",
    "its orientation to represent the instantiation parameters. Active capsules at one level\n",
    "make predictions, via transformation matrices, for the instantiation parameters of\n",
    "higher-level capsules. When multiple predictions agree, a higher level capsule\n",
    "becomes active. We show that a discrimininatively trained, multi-layer capsule\n",
    "system achieves state-of-the-art performance on MNIST and is considerably better\n",
    "than a convolutional net at recognizing highly overlapping digits. To achieve these\n",
    "results we use an iterative routing-by-agreement mechanism: A lower-level capsule\n",
    "prefers to send its output to higher level capsules whose activity vectors have a big\n",
    "scalar product with the prediction coming from the lower-level capsule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![capsulenet](./images/capsulenetwork.png \"Capsule Network Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![capsulenet](./images/capsulenet-reconstruction.png \"Capsule Network Reconstruction Arch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Get capsule working\n",
    "- [x] Study normal behavior on MNIST\n",
    "- [x] Study scaling invariance behavior\n",
    "- [x] Study rotation invariance behavior\n",
    "- [ ] Retrain on AffineMNIST ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![capsulenet](./images/capsule_rotation_study.png \"Capsule rotation invariance study (0-360) degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![capsulenet](./images/capsule_scaling_study.png \"Capsule scale invariance study (10-300) %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "import math\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "#from skimage.transform.radon_transform import fft\n",
    "from scipy import fftpack\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchnet.engine import Engine\n",
    "from torchnet.logger import VisdomPlotLogger, VisdomLogger\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from tqdm import tqdm\n",
    "import torchnet as tnt\n",
    "\n",
    "\n",
    "#@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))\n",
    "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
    "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
    "import matplotlib.axes as axes;\n",
    "from matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "from IPython.core.pylabtools import figsize\n",
    "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
    "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
    "%config InlineBackend.figure_format = notebook_screen_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 300\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 20\n",
    "NUM_ROUTING_ITERATIONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(input, dim=1):\n",
    "    transposed_input = input.transpose(dim, len(input.size()) - 1)\n",
    "    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)\n",
    "\n",
    "\n",
    "def augmentation(x, max_shift=2):\n",
    "    _, _, height, width = x.size()\n",
    "\n",
    "    h_shift, w_shift = np.random.randint(-max_shift, max_shift + 1, size=2)\n",
    "    source_height_slice = slice(max(0, h_shift), h_shift + height)\n",
    "    source_width_slice = slice(max(0, w_shift), w_shift + width)\n",
    "    target_height_slice = slice(max(0, -h_shift), -h_shift + height)\n",
    "    target_width_slice = slice(max(0, -w_shift), -w_shift + width)\n",
    "\n",
    "    shifted_image = torch.zeros(*x.size())\n",
    "    shifted_image[:, :, source_height_slice, source_width_slice] = x[:, :, target_height_slice, target_width_slice]\n",
    "    return shifted_image.float()\n",
    "\n",
    "\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels, kernel_size=None, stride=None,\n",
    "                 num_iterations=NUM_ROUTING_ITERATIONS):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "\n",
    "        self.num_route_nodes = num_route_nodes\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        if num_route_nodes != -1:\n",
    "            self.route_weights = nn.Parameter(torch.randn(num_capsules, num_route_nodes, in_channels, out_channels))\n",
    "        else:\n",
    "            self.capsules = nn.ModuleList(\n",
    "                [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0) for _ in\n",
    "                 range(num_capsules)])\n",
    "\n",
    "    def squash(self, tensor, dim=-1):\n",
    "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)\n",
    "        return scale * tensor / torch.sqrt(squared_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_route_nodes != -1:\n",
    "            priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :]\n",
    "\n",
    "            logits = Variable(torch.zeros(*priors.size())).cuda()\n",
    "            for i in range(self.num_iterations):\n",
    "                probs = softmax(logits, dim=2)\n",
    "                outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "\n",
    "                if i != self.num_iterations - 1:\n",
    "                    delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                    logits = logits + delta_logits\n",
    "        else:\n",
    "            outputs = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n",
    "            outputs = torch.cat(outputs, dim=-1)\n",
    "            outputs = self.squash(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9, stride=1)\n",
    "        self.primary_capsules = CapsuleLayer(num_capsules=8, num_route_nodes=-1, in_channels=256, out_channels=32,\n",
    "                                             kernel_size=9, stride=2)\n",
    "        self.digit_capsules = CapsuleLayer(num_capsules=NUM_CLASSES, num_route_nodes=32 * 6 * 6, in_channels=8,\n",
    "                                           out_channels=16)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16 * NUM_CLASSES, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = F.relu(self.conv1(x), inplace=True)\n",
    "        x = self.primary_capsules(x)\n",
    "        x = self.digit_capsules(x).squeeze().transpose(0, 1)\n",
    "\n",
    "        classes = (x ** 2).sum(dim=-1) ** 0.5\n",
    "        classes = F.softmax(classes, dim=-1)\n",
    "        if y is None:\n",
    "            # In all batches, get the most active capsule.\n",
    "            _, max_length_indices = classes.max(dim=1) #something wrong here which I have no clue about\n",
    "            y = Variable(torch.eye(NUM_CLASSES)).cuda().index_select(dim=0, index=max_length_indices.data)\n",
    "\n",
    "        reconstructions = self.decoder((x * y[:, :, None]).view(x.size(0), -1))\n",
    "\n",
    "        return classes, reconstructions\n",
    "\n",
    "\n",
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "        self.reconstruction_loss = nn.MSELoss(size_average=False)\n",
    "\n",
    "    def forward(self, images, labels, classes, reconstructions):\n",
    "        left = F.relu(0.9 - classes, inplace=True) ** 2\n",
    "        right = F.relu(classes - 0.1, inplace=True) ** 2\n",
    "\n",
    "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
    "        margin_loss = margin_loss.sum()\n",
    "\n",
    "        assert torch.numel(images) == torch.numel(reconstructions)\n",
    "        images = images.view(reconstructions.size()[0], -1)\n",
    "        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
    "\n",
    "        return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)\n",
    "    \n",
    "class CustomScaling(object):\n",
    "    \"\"\"Rotate image by a fixed angle which is ready for tranform.Compose()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, angle=0, translate=[0,0], shear=0):\n",
    "        self.scale = scale\n",
    "        self.angle = angle\n",
    "        self.translate = translate\n",
    "        self.shear = shear\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        return transforms.ToTensor()(\n",
    "            transforms.functional.affine(\n",
    "                transforms.ToPILImage()(img), \n",
    "                self.angle, self.translate, self.scale, self.shear))\n",
    "\n",
    "class CustomRotation(object):\n",
    "    \"\"\"Rotate image by a fixed angle which is ready for tranform.Compose()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees, resample=False, expand=False, center=None):\n",
    "        self.degrees = degrees\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = center\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        return transforms.ToTensor()(\n",
    "            transforms.functional.rotate(\n",
    "                transforms.ToPILImage()(img), \n",
    "                self.degrees, self.resample, self.expand, self.center))\n",
    "    \n",
    "def rotate_tensor(_in_tensor, plot=True):\n",
    "    \"\"\"\n",
    "    Usage: number, number_90, number_180, number_270 = rotate_tensor(example_data[15])\n",
    "    \"\"\"\n",
    "    in_tensor = _in_tensor.clone()\n",
    "    # Add one more channel to the beginning. Tensor shape = 1,1,28,28\n",
    "    in_tensor.unsqueeze_(0)\n",
    "    # Convert to Pytorch variable\n",
    "    in_tensor = Variable(in_tensor, requires_grad=True)\n",
    "    \n",
    "    in_tensor_90 = in_tensor.transpose(2, 3).flip(3)\n",
    "    in_tensor_180 = in_tensor.flip(2).flip(3)\n",
    "    in_tensor_270 = in_tensor.transpose(2, 3).flip(2)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(1)\n",
    "        plt.subplot(221)\n",
    "        plt.gca().set_title('0 degree')\n",
    "        plt.imshow(in_tensor[0][0].cpu().detach().clone(), cmap='gray')\n",
    "        plt.subplot(222)\n",
    "        plt.gca().set_title('+90 degree')\n",
    "        plt.imshow(in_tensor_90[0][0].cpu().detach().clone(), cmap='gray')\n",
    "        plt.subplot(223)\n",
    "        plt.gca().set_title('+270 degree')\n",
    "        plt.imshow(in_tensor_270[0][0].cpu().detach().clone(), cmap='gray')\n",
    "        plt.subplot(224)\n",
    "        plt.gca().set_title('+180 degree')\n",
    "        plt.imshow(in_tensor_180[0][0].cpu().detach().clone(), cmap='gray')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return(in_tensor, in_tensor_90, in_tensor_180, in_tensor_270)\n",
    "\n",
    "def custom_viz(kernels, path=None, cols=None, size=None, verbose=False, axis=False):\n",
    "    \"\"\"Visualize weight and activation matrices learned \n",
    "    during the optimization process. Works for any size of kernels.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    kernels: Weight or activation matrix. Must be a high dimensional\n",
    "    Numpy array. Tensors will not work.\n",
    "    path: Path to save the visualizations.\n",
    "    cols: Number of columns (doesn't work completely yet.)\n",
    "    size: Tuple input for size. For example: size=(5,5)\n",
    "    verbose: Print information about the input.\n",
    "    Example\n",
    "    =======\n",
    "    kernels = model.conv1.weight.cpu().detach().clone()\n",
    "    kernels = kernels - kernels.min()\n",
    "    kernels = kernels / kernels.max()\n",
    "    custom_viz(kernels, 'results/conv1_weights.png', 5)\n",
    "    \"\"\"\n",
    "    def set_size(w,h, ax=None):\n",
    "        \"\"\" w, h: width, height in inches \"\"\"\n",
    "        if not ax: ax=plt.gca()\n",
    "        l = ax.figure.subplotpars.left\n",
    "        r = ax.figure.subplotpars.right\n",
    "        t = ax.figure.subplotpars.top\n",
    "        b = ax.figure.subplotpars.bottom\n",
    "        figw = float(w)/(r-l)\n",
    "        figh = float(h)/(t-b)\n",
    "        ax.figure.set_size_inches(figw, figh)\n",
    "    \n",
    "    N = kernels.shape[0]\n",
    "    C = kernels.shape[1]\n",
    "    total_cols = N*C\n",
    "    pos = range(1,total_cols + 1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Shape of input: \", kernels.shape)\n",
    "        \n",
    "    if cols==None:\n",
    "        req_cols = C\n",
    "        num_rows = N\n",
    "    elif cols:\n",
    "        req_cols = cols\n",
    "        # Account for more rows while diving total cols\n",
    "        # with requested number of cols in the figure\n",
    "        # Hence, using np.ceil to get the largest int\n",
    "        # from the quotient of division.\n",
    "        num_rows = int(np.ceil(total_cols/req_cols))\n",
    "    elif C>1:\n",
    "        # Check for 1D arrays and such. Mostly not needed.\n",
    "        req_cols = C\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    fig.tight_layout()\n",
    "    k=0\n",
    "    for i in range(kernels.shape[0]):\n",
    "        for j in range(kernels.shape[1]):\n",
    "            img = kernels[i][j]\n",
    "            ax = fig.add_subplot(num_rows,req_cols,pos[k])\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            if axis:\n",
    "                plt.axis('on')\n",
    "            elif axis==False:\n",
    "                plt.axis('off')\n",
    "            k = k+1\n",
    "    if size:\n",
    "        size_h,size_w = size\n",
    "        set_size(size_h,size_w,ax)\n",
    "    if path:\n",
    "        plt.savefig(path, dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "def clean_up_variable(starts_with=None):\n",
    "    for name in dir():\n",
    "        if name.startswith(str(starts_with)):\n",
    "            del globals()[name]\n",
    "\n",
    "    for name in dir():\n",
    "        if name.startswith(str(starts_with)):\n",
    "            del locals()[name]\n",
    "#     except TypeError:\n",
    "#         print(\"ERR: Please pass first few letters of the variables you need to delete as an argument\")\n",
    "        \n",
    "def test_capsulenet(_model, _test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the test performance of capsulenet\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    for batch_idx, (ground_truth, example_targets) in enumerate(_test_loader):\n",
    "        output_label, _ = _model(Variable(ground_truth).cuda())\n",
    "        equality = (example_targets == torch.exp(output_label).max(dim=1)[1].cpu())\n",
    "        # print(equality)\n",
    "        acc += equality.type(torch.FloatTensor).mean()\n",
    "    return acc/float(batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters: 8215568\n"
     ]
    }
   ],
   "source": [
    "model = CapsuleNet()\n",
    "model.load_state_dict(torch.load('epochs/epoch_21.pt'))\n",
    "model.cuda()\n",
    "\n",
    "print(\"# parameters:\", sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 20, 20]          20,992\n",
      "            Conv2d-2             [-1, 32, 6, 6]         663,584\n",
      "            Conv2d-3             [-1, 32, 6, 6]         663,584\n",
      "            Conv2d-4             [-1, 32, 6, 6]         663,584\n",
      "            Conv2d-5             [-1, 32, 6, 6]         663,584\n",
      "            Conv2d-6             [-1, 32, 6, 6]         663,584\n",
      "            Conv2d-7             [-1, 32, 6, 6]         663,584\n",
      "            Conv2d-8             [-1, 32, 6, 6]         663,584\n",
      "            Conv2d-9             [-1, 32, 6, 6]         663,584\n",
      "     CapsuleLayer-10              [-1, 1152, 8]               0\n",
      "     CapsuleLayer-11          [-1, 2, 1, 1, 16]               0\n",
      "           Linear-12                  [-1, 512]          82,432\n",
      "             ReLU-13                  [-1, 512]               0\n",
      "           Linear-14                 [-1, 1024]         525,312\n",
      "             ReLU-15                 [-1, 1024]               0\n",
      "           Linear-16                  [-1, 784]         803,600\n",
      "          Sigmoid-17                  [-1, 784]               0\n",
      "================================================================\n",
      "Total params: 6,741,008\n",
      "Trainable params: 6,741,008\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.96\n",
      "Params size (MB): 25.71\n",
      "Estimated Total Size (MB): 26.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CapsuleNet(\n",
       "  (conv1): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (primary_capsules): CapsuleLayer(\n",
       "    (capsules): ModuleList(\n",
       "      (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (1): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (2): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (3): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (4): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (5): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (6): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "      (7): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (digit_capsules): CapsuleLayer()\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=1024, out_features=784, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iterator(mode):\n",
    "    dataset = MNIST(root='./data', download=True, train=mode)\n",
    "    data = getattr(dataset, 'train_data' if mode else 'data')\n",
    "    labels = getattr(dataset, 'train_labels' if mode else 'targets')\n",
    "    tensor_dataset = tnt.dataset.TensorDataset([data, labels])\n",
    "\n",
    "    return tensor_dataset.parallel(batch_size=BATCH_SIZE, num_workers=4, shuffle=mode)\n",
    "\n",
    "test_sample = next(iter(get_iterator(False)))\n",
    "\n",
    "ground_truth = (test_sample[0].unsqueeze(1).float() / 255.0)\n",
    "output_label, reconstructions = model(Variable(ground_truth).cuda())\n",
    "reconstruction = reconstructions.cpu().view_as(ground_truth).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_id = np.random.randint(100, size=1)[0]\n",
    "print(\"Predicted Class: \", np.argmax(output_label[image_id].cpu().detach().numpy()))\n",
    "plt.imshow(test_sample[0][image_id].unsqueeze(0).cuda().cpu().detach().numpy()[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction as a regularization method\n",
    "\n",
    "> We use an additional reconstruction loss to encourage the digit capsules to encode the instantiation\n",
    "parameters of the input digit. During training, we mask out all but the activity vector of the correct\n",
    "digit capsule. Then we use this activity vector to reconstruct the input image. The output of the digit\n",
    "capsule is fed into a decoder consisting of 3 fully connected layers that model the pixel intensitie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring our own train and test loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default data loaders\n",
    "orig_train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "orig_test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "orig_examples = enumerate(orig_test_loader)\n",
    "orig_batch_idx, (orig_ground_truth, orig_example_targets) = next(orig_examples)\n",
    "\n",
    "orig_output_label, orig_reconstructions = model(Variable(orig_ground_truth).cuda())\n",
    "orig_reconstruction = orig_reconstructions.cpu().view_as(orig_ground_truth).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Class:  7 || Predicted Class:  7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 202,
       "width": 410
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_id = np.random.randint(BATCH_SIZE, size=1)[0]\n",
    "\n",
    "print(\"Actual Class: \", int(orig_example_targets[image_id].cpu().detach().numpy()), \"|| Predicted Class: \", np.argmax(orig_output_label[image_id].cpu().detach().numpy()))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(orig_ground_truth[image_id].cuda().cpu().detach().numpy()[0], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(orig_reconstruction[image_id].cuda().cpu().detach().numpy()[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_acc = test_capsulenet(model, orig_test_loader)\n",
    "print(\"Test Accuracy for original images is: \", orig_acc.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up_variable('orig')\n",
    "# %who_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Test ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.9 # Specifies the scaling factor of images.\n",
    "\n",
    "# Define the train and test loader\n",
    "# Here we are adding our CustomScaling function to the transformations\n",
    "scale_train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "scale_test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "scale_examples = enumerate(scale_test_loader)\n",
    "scale_batch_idx, (scale_ground_truth, scale_example_targets) = next(scale_examples)\n",
    "\n",
    "# ground_truth = (test_sample[0].unsqueeze(1).float() / 255.0)\n",
    "scale_output_label, scale_reconstructions = model(Variable(scale_ground_truth).cuda())\n",
    "scale_reconstruction = scale_reconstructions.cpu().view_as(scale_ground_truth).data\n",
    "scale_acc = test_capsulenet(model, scale_test_loader)\n",
    "\n",
    "print(\"Test Accuracy for %d percent scaled images is: %.4f\" %(scale*100, scale_acc.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = np.random.randint(BATCH_SIZE, size=1)[0]\n",
    "\n",
    "print(\"Actual Class: \", int(scale_example_targets[image_id].cpu().detach().numpy()), \"|| Predicted Class: \", np.argmax(scale_output_label[image_id].cpu().detach().numpy()))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(scale_ground_truth[image_id].cuda().cpu().detach().numpy()[0], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(scale_reconstruction[image_id].cuda().cpu().detach().numpy()[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOP FOR EASY STUDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_acc_study = []\n",
    "scales = np.arange(0.1, 3.1, 0.1)\n",
    "\n",
    "for i in scales:\n",
    "    scale = round(i,2) # Specifies the scaling factor of images.\n",
    "\n",
    "    # Define the train and test loader\n",
    "    # Here we are adding our CustomScaling function to the transformations\n",
    "    scale_train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data/', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           CustomScaling(scale),\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    scale_test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           CustomScaling(scale),\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    scale_examples = enumerate(scale_test_loader)\n",
    "    scale_batch_idx, (scale_ground_truth, scale_example_targets) = next(scale_examples)\n",
    "\n",
    "    # ground_truth = (test_sample[0].unsqueeze(1).float() / 255.0)\n",
    "    scale_output_label, scale_reconstructions = model(Variable(scale_ground_truth).cuda())\n",
    "    scale_reconstruction = scale_reconstructions.cpu().view_as(scale_ground_truth).data\n",
    "    scale_acc = test_capsulenet(model, scale_test_loader)\n",
    "    scale_acc_study.append(scale_acc)\n",
    "    print(\"Test Accuracy for %d percent scaled images is: %.4f\" %(scale*100, scale_acc.cpu().detach().numpy()))\n",
    "np.savetxt(fname=\"capsule_scaling_acc_values.txt\", X=scale_acc_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for variation in scales\n",
    "plt.plot(scales, scale_acc_study)\n",
    "plt.xlabel('Scale', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Capsule Scaling Study - Scales (10 - 300)\")\n",
    "plt.savefig('images/capsule_scaling_study.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOP ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dir():\n",
    "    if name.startswith('rota'):\n",
    "        del globals()[name]\n",
    "for name in dir():\n",
    "    if name.startswith('Custom'):\n",
    "        del globals()[name]\n",
    "%who_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up_variable('scale')\n",
    "%who_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation Test ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = 20 # Specifies the scaling factor of images.\n",
    "\n",
    "# Define the train and test loader\n",
    "# Here we are adding our CustomRotation function to the transformations\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomRotation(rotation),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomRotation(rotation),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "## try out stuff\n",
    "\n",
    "# transforms.functional.affine(img=transforms.functional.to_pil_image(example_data[0]),\n",
    "#                              angle=0, translate=(0,0),\n",
    "#                              scale=0.4, shear=0)\n",
    "\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (ground_truth, example_targets) = next(examples)\n",
    "\n",
    "# ground_truth = (test_sample[0].unsqueeze(1).float() / 255.0)\n",
    "output_label, reconstructions = model(Variable(ground_truth).cuda())\n",
    "reconstruction = reconstructions.cpu().view_as(ground_truth).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for 0 percent scaled images is: 0.9948\n",
      "Test Accuracy for 10 percent scaled images is: 0.9882\n",
      "Test Accuracy for 20 percent scaled images is: 0.9632\n",
      "Test Accuracy for 30 percent scaled images is: 0.8810\n",
      "Test Accuracy for 40 percent scaled images is: 0.7290\n",
      "Test Accuracy for 50 percent scaled images is: 0.5219\n",
      "Test Accuracy for 60 percent scaled images is: 0.3416\n",
      "Test Accuracy for 70 percent scaled images is: 0.2352\n",
      "Test Accuracy for 80 percent scaled images is: 0.1748\n",
      "Test Accuracy for 90 percent scaled images is: 0.1572\n",
      "Test Accuracy for 100 percent scaled images is: 0.1689\n",
      "Test Accuracy for 110 percent scaled images is: 0.1891\n",
      "Test Accuracy for 120 percent scaled images is: 0.2352\n",
      "Test Accuracy for 130 percent scaled images is: 0.3017\n",
      "Test Accuracy for 140 percent scaled images is: 0.3594\n",
      "Test Accuracy for 150 percent scaled images is: 0.4088\n",
      "Test Accuracy for 160 percent scaled images is: 0.4436\n",
      "Test Accuracy for 170 percent scaled images is: 0.4650\n",
      "Test Accuracy for 180 percent scaled images is: 0.4715\n",
      "Test Accuracy for 190 percent scaled images is: 0.4692\n",
      "Test Accuracy for 200 percent scaled images is: 0.4525\n",
      "Test Accuracy for 210 percent scaled images is: 0.4075\n",
      "Test Accuracy for 220 percent scaled images is: 0.3611\n",
      "Test Accuracy for 230 percent scaled images is: 0.3046\n",
      "Test Accuracy for 240 percent scaled images is: 0.2508\n",
      "Test Accuracy for 250 percent scaled images is: 0.2044\n",
      "Test Accuracy for 260 percent scaled images is: 0.1658\n",
      "Test Accuracy for 270 percent scaled images is: 0.1472\n",
      "Test Accuracy for 280 percent scaled images is: 0.1454\n",
      "Test Accuracy for 290 percent scaled images is: 0.1690\n",
      "Test Accuracy for 300 percent scaled images is: 0.2436\n",
      "Test Accuracy for 310 percent scaled images is: 0.4073\n",
      "Test Accuracy for 320 percent scaled images is: 0.6374\n",
      "Test Accuracy for 330 percent scaled images is: 0.8484\n",
      "Test Accuracy for 340 percent scaled images is: 0.9576\n",
      "Test Accuracy for 350 percent scaled images is: 0.9892\n",
      "Test Accuracy for 360 percent scaled images is: 0.9948\n"
     ]
    }
   ],
   "source": [
    "rotation_acc_study = []\n",
    "rotations = np.arange(0, 370, 10)\n",
    "\n",
    "for rotation in rotations:\n",
    "    # Define the train and test loader\n",
    "    # Here we are adding our CustomRotation function to the transformations\n",
    "    rotation_train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data/', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           CustomRotation(rotation),\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    rotation_test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           CustomRotation(rotation),\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    rotation_examples = enumerate(rotation_test_loader)\n",
    "    rotation_batch_idx, (rotation_ground_truth, rotation_example_targets) = next(rotation_examples)\n",
    "\n",
    "    # ground_truth = (test_sample[0].unsqueeze(1).float() / 255.0)\n",
    "    rotation_output_label, rotation_reconstructions = model(Variable(rotation_ground_truth).cuda())\n",
    "    rotation_reconstruction = rotation_reconstructions.cpu().view_as(rotation_ground_truth).data\n",
    "    rotation_acc = test_capsulenet(model, rotation_test_loader)\n",
    "    rotation_acc_study.append(rotation_acc)\n",
    "    print(\"Test Accuracy for %d percent scaled images is: %.4f\" %(rotation, rotation_acc.cpu().detach().numpy()))\n",
    "np.savetxt(fname=\"capsule_rotation_acc_values.txt\", X=rotation_acc_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for variation in scales\n",
    "fig = plt.Figure()\n",
    "plt.plot(rotations, rotation_acc_study)\n",
    "plt.xlabel('Rotation', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Capsule Rotation Study - Angles (0 -360)\")\n",
    "plt.savefig('images/capsule_rotation_study.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "![rot_study](./images/capsule_rotation_study.png \"Capsule Rotation Study\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "![scaling_study](./images/capsule_scaling_study.png \"Capsule Scaling Study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
