{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Arun Das\n",
    "    Research Fellow\n",
    "    Secure AI and Autonomy Laboratory\n",
    "    University of Texas at San Antonio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Invariance in CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the course of history, convolution operation has helped accelerate science and signal processing in a variety of ways. With the advent of deep learning, computer vision researchers began exploring the use of 2D and 3D convolutional neural networks (CNNs) directly on 2D or 3D images to reduce the parameters involved with fully connected deep neural networks. With large amount of data and computation at their disposal, supervised CNN learning algorithms tackled problems which were almost impossible to generalize in the past decade.\n",
    "\n",
    "CNNs are impressive feature extractors, extracting features heirarchically from the training images during the learning process. First few layers close to the input data learns kernels related to high contrast points, edges, and lines. Layers further in the network learns to map these primitive kernels together to understand countours and other shapes. This heirarchical way of learning by representation enables complex pattern recognition that was impossible using traditional signal processing and machine learning algorithms.\n",
    "\n",
    "Invariances in input data distribution used for training is mapped in to the CNN as weights, which are infact learned by the kernels. For example, if a face classifier is trained on images with face cropped, aligned, and centered in the center of the image, the CNN will learn to map the input pixels accordingly, and generalize on providing impressive results on faces which are preprocessed and centered properly. However, the interesting question arises on the robustness of CNNs on slighly invariant input images which are from outside the data distribution. This is where our discussion on invariance starts - and in my opinion, the many questions we ask are translated from this bigger topic of robustness and safe artificial intelligence (AI).\n",
    "\n",
    "For the scope of this study, we specifically focus on scale invariance issues of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import math\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "#from skimage.transform.radon_transform import fft\n",
    "from scipy import fftpack\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the hyperparameters as keys in an `args` dictionary. This way, it is easy to add and remove hyperparameters, and also to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PsdeLhYhAQl"
   },
   "outputs": [],
   "source": [
    "args={}\n",
    "kwargs={}\n",
    "args['batch_size']=1000\n",
    "args['test_batch_size']=1000\n",
    "args['epochs']=20  # The number of Epochs is the number of times you go \n",
    "                   # through the full dataset. \n",
    "args['lr']=0.01 # Learning rate is how fast it will decend. \n",
    "args['momentum']=0.5 # SGD momentum (default: 0.5) Momentum is a moving \n",
    "                     # average of our gradients (helps to keep direction).\n",
    "\n",
    "args['seed']=1 # random seed\n",
    "args['log_interval']=40\n",
    "args['cuda']=True # False if you don't have a CUDA w/ NVIDIA GPU available.\n",
    "args['train_now']=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom scaling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScaling(object):\n",
    "    \"\"\"Rotate image by a fixed angle which is ready for tranform.Compose()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, angle=0, translate=[0,0], shear=0):\n",
    "        self.scale = scale\n",
    "        self.angle = angle\n",
    "        self.translate = translate\n",
    "        self.shear = shear\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        return transforms.ToTensor()(\n",
    "            transforms.functional.affine(\n",
    "                transforms.ToPILImage()(img), \n",
    "                self.angle, self.translate, self.scale, self.shear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale to 45% of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):          \n",
    "     \n",
    "    def __init__(self):     \n",
    "        super(LeNet5, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given \n",
    "        # as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, \n",
    "                                     kernel_size=5, stride=1, padding=2)\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, \n",
    "                                     kernel_size=5, stride=1, padding=2)\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, \n",
    "                                     kernel_size=5, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(7*7*120, 120)\n",
    "        # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
    "        self.fc2 = nn.Linear(120, 84)       \n",
    "        # convert matrix with 120 features to a matrix of 84 features (columns)\n",
    "        self.fc3 = nn.Linear(84, 10)        \n",
    "        # convert matrix with 84 features to a matrix of 10 features (columns)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        # max-pooling with 2x2 grid \n",
    "        x = self.max_pool_1(x) \n",
    "        # Conv2 + ReLU\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_2(x)\n",
    "        # Conv3 + ReLU\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 7*7*120)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # FC-3\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             156\n",
      "         MaxPool2d-2            [-1, 6, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 14, 14]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 7, 7]               0\n",
      "            Conv2d-5            [-1, 120, 7, 7]          48,120\n",
      "            Linear-6                  [-1, 120]         705,720\n",
      "            Linear-7                   [-1, 84]          10,164\n",
      "            Linear-8                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 767,426\n",
      "Trainable params: 767,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 2.93\n",
      "Estimated Total Size (MB): 3.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "if args['cuda']:\n",
    "    model.cuda()\n",
    "\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8AeVTCYhAQo",
    "outputId": "af98bb02-b673-44df-e26a-b15d99b29965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7d9d2a3710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale = 0.45 # Specifies the scaling factor of images.\n",
    "\n",
    "# Define the train and test loader\n",
    "# Here we are adding our CustomRotation function to the transformations\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['test_batch_size'], shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "## try out stuff\n",
    "\n",
    "# transforms.functional.affine(img=transforms.functional.to_pil_image(example_data[0]),\n",
    "#                              angle=0, translate=(0,0),\n",
    "#                              scale=0.4, shear=0)\n",
    "\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(\"Predicted Class: \", \n",
    "      np.argmax(model.forward(example_data[0].unsqueeze_(0).cuda()).cpu().detach().numpy()))\n",
    "\n",
    "plt.imshow(example_data[0].cuda().cpu().detach().numpy()[0], cmap='gray')\n",
    "# transforms.functional.to_pil_image(example_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #Variables in Pytorch are differenciable. \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #This will zero out the gradients for this batch. \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Calculate the loss The negative log likelihood loss. \n",
    "        # It is useful to train a classification problem with C classes.\n",
    "        loss = F.nll_loss(output, target)\n",
    "        #dloss/dx for every Variable \n",
    "        loss.backward()\n",
    "        #to do a one-step update on our parameter.\n",
    "        optimizer.step()\n",
    "        #Print out the loss periodically. \n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        with torch.no_grad(): # volatile was removed and now \n",
    "            # has no effect. Use `with torch.no_grad():` instead.\n",
    "            data= Variable(data)\n",
    "        target = Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss # size_average and reduce args will \n",
    "        # be deprecated, please use reduction='sum' instead.\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').data \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1] \n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN model on normal MNIST images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use stocastic gradient descend (SGD) as the optimizer and use momentum to lead the way. The hyperparameters are passed using `args` dictionary and the required key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rmcWuY4hAQ0",
    "outputId": "62e301f7-864a-4d11-872e-2277c0d82fea"
   },
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), \n",
    "#                       lr=args['lr'], momentum=args['momentum'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "\n",
    "# Training loop. \n",
    "# Change `args['log_interval']` if you want to change logging behavior.\n",
    "# We test the network in each epoch.\n",
    "# Setting the bool `args['train_now']` to not run training all the time.\n",
    "# We'll save the weights and use the saved weights instead of \n",
    "# training the network everytime we load the jupyter notebook.\n",
    "\n",
    "\n",
    "args['train_now'] = False\n",
    "args['epochs'] = 30\n",
    "\n",
    "if args['train_now']:\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    torch.save(model.state_dict(), 'models/lenet5_normal_mnist.pytrh')\n",
    "else:\n",
    "    if args['cuda']:\n",
    "        device = torch.device(\"cuda\")\n",
    "        model.load_state_dict(torch.load('models/lenet5_normal_mnist.pytrh'))\n",
    "        model.to(device)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load('models/lenet5_normal_mnist.pytrh'))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel weight visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to understand how the network learns, it is not only important to log the training and testing accuracies but also to visualize what the network learns. As we get over the deep learning hype, we should invest time in learning the intricate features which makes these networks what they are. As a first step, we shall write a custom visualization function to plot the kernels and activations of the CNN - whatever the size. This is a key piece of code that will drive us forward and unfortunately isn't available in Pytorch or internet :) So custom indeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_boxplot(kernels, path=None, cols=None, size=None, verbose=False):\n",
    "    \"\"\"Statistical analysis using BoxPlot for weight and activation matrices\n",
    "    learned during the optimization process. Works for any size of kernels.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    kernels: Weight or activation matrix. Must be a high dimensional\n",
    "    Numpy array. Tensors will not work.\n",
    "    path: Path to save the visualizations.\n",
    "    cols: Number of columns (doesn't work completely yet.)\n",
    "    size: Tuple input for size. For example: size=(5,5)\n",
    "    verbose: Print information about the input.\n",
    "    Example\n",
    "    =======\n",
    "    kernels = model.conv1.weight.cpu().detach().clone()\n",
    "    kernels = kernels - kernels.min()\n",
    "    kernels = kernels / kernels.max()\n",
    "    custom_boxplot(kernels, 'results/conv1_weights_boxplot.png', 5, size=(25,5))\n",
    "    \"\"\"\n",
    "    def set_size(w,h, ax=None):\n",
    "        \"\"\" w, h: width, height in inches \"\"\"\n",
    "        if not ax: ax=plt.gca()\n",
    "        l = ax.figure.subplotpars.left\n",
    "        r = ax.figure.subplotpars.right\n",
    "        t = ax.figure.subplotpars.top\n",
    "        b = ax.figure.subplotpars.bottom\n",
    "        figw = float(w)/(r-l)\n",
    "        figh = float(h)/(t-b)\n",
    "        ax.figure.set_size_inches(figw, figh)\n",
    "\n",
    "    kernelshape = kernels.shape\n",
    "    if verbose:\n",
    "        print(\"Shape of input kernel: \", kernelshape)\n",
    "        \n",
    "    if cols==None:\n",
    "        cols = 6\n",
    "        rows = np.int(np.ceil(kernelshape[0]/cols))\n",
    "\n",
    "    pos = range(1, kernelshape[0]+1)\n",
    "    k=0\n",
    "    fig = plt.figure(1)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    for i in range(kernelshape[0]):\n",
    "        ax = fig.add_subplot(rows,cols,pos[k])\n",
    "        w_vol = np.reshape(kernels[k].cpu().detach().clone().numpy(), (kernelshape[1], kernelshape[2]*kernelshape[3]))\n",
    "        w_vol_df = pd.DataFrame(w_vol.T)\n",
    "        if verbose:\n",
    "            msd = zip(w_vol_df.mean(), w_vol_df.std())\n",
    "            for i, values in enumerate(msd):\n",
    "                print(\"For kernel Volume %d\" %i)\n",
    "                print(\"Mean+-SD: %0.2f+-%0.2f\" %values)\n",
    "                print('----------------------')\n",
    "        w_vol_df.boxplot(ax=ax)\n",
    "        title_boxplot = 'Kernel ' + str(i)\n",
    "        plt.title( title_boxplot )\n",
    "        k+=1\n",
    "        if k==kernelshape:\n",
    "            break\n",
    "    if size:\n",
    "        size_h,size_w = size\n",
    "        set_size(size_h,size_w,ax)\n",
    "    if path:\n",
    "        plt.savefig(path, dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_viz(kernels, path=None, cols=None, size=None, verbose=False, axis=False, cmap='gray'):\n",
    "    \"\"\"Visualize weight and activation matrices learned \n",
    "    during the optimization process. Works for any size of kernels.\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    kernels: Weight or activation matrix. Must be a high dimensional\n",
    "    Numpy array. Tensors will not work.\n",
    "    path: Path to save the visualizations.\n",
    "    cols: Number of columns (doesn't work completely yet.)\n",
    "    size: Tuple input for size. For example: size=(5,5)\n",
    "    verbose: Print information about the input.\n",
    "    axis: Plot axis for images.\n",
    "    cmap: Color map for output images.\n",
    "    Example\n",
    "    =======\n",
    "    kernels = model.conv1.weight.cpu().detach().clone()\n",
    "    kernels = kernels - kernels.min()\n",
    "    kernels = kernels / kernels.max()\n",
    "    custom_viz(kernels, 'results/conv1_weights.png', 5)\n",
    "    \"\"\"\n",
    "    def set_size(w,h, ax=None):\n",
    "        \"\"\" w, h: width, height in inches \"\"\"\n",
    "        if not ax: ax=plt.gca()\n",
    "        l = ax.figure.subplotpars.left\n",
    "        r = ax.figure.subplotpars.right\n",
    "        t = ax.figure.subplotpars.top\n",
    "        b = ax.figure.subplotpars.bottom\n",
    "        figw = float(w)/(r-l)\n",
    "        figh = float(h)/(t-b)\n",
    "        ax.figure.set_size_inches(figw, figh)\n",
    "    \n",
    "    N = kernels.shape[0]\n",
    "    C = kernels.shape[1]\n",
    "    total_cols = N*C\n",
    "    pos = range(1,total_cols + 1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Shape of input: \", kernels.shape)\n",
    "        \n",
    "    if cols==None:\n",
    "        req_cols = C\n",
    "        num_rows = N\n",
    "    elif cols:\n",
    "        req_cols = cols\n",
    "        # Account for more rows while diving total cols\n",
    "        # with requested number of cols in the figure\n",
    "        # Hence, using np.ceil to get the largest int\n",
    "        # from the quotient of division.\n",
    "        num_rows = int(np.ceil(total_cols/req_cols))\n",
    "    elif C>1:\n",
    "        # Check for 1D arrays and such. Mostly not needed.\n",
    "        req_cols = C\n",
    "\n",
    "    fig = plt.figure(1)\n",
    "    fig.tight_layout()\n",
    "    k=0\n",
    "    for i in range(kernels.shape[0]):\n",
    "        for j in range(kernels.shape[1]):\n",
    "            img = kernels[i][j]\n",
    "            ax = fig.add_subplot(num_rows,req_cols,pos[k])\n",
    "            if cmap:\n",
    "                ax.imshow(img, cmap=cmap)\n",
    "            else:\n",
    "                ax.imshow(img)\n",
    "            if axis:\n",
    "                plt.axis('on')\n",
    "            elif axis==False:\n",
    "                plt.axis('off')\n",
    "            k = k+1\n",
    "    if size:\n",
    "        size_h,size_w = size\n",
    "        set_size(size_h,size_w,ax)\n",
    "    if path:\n",
    "        plt.savefig(path, dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class:  7\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "print(\"Predicted Class: \", \n",
    "      np.argmax(model.forward(example_data[0].unsqueeze_(0).cuda()).cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_data[0].unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperLeNet5(nn.Module):          \n",
    "     \n",
    "    def __init__(self):     \n",
    "        super(SuperLeNet5, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given \n",
    "        # as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, \n",
    "                                     kernel_size=9, stride=1, padding=2)\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, \n",
    "                                     kernel_size=7, stride=1, padding=2)\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, \n",
    "                               kernel_size=5, stride=1, padding=2)\n",
    "        # conv for 2nd branch\n",
    "        self.b2conv1 = nn.Conv2d(in_channels=1, out_channels=6, \n",
    "                                     kernel_size=5, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(5*5*120, 120)\n",
    "        # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
    "        self.fc2 = nn.Linear(120, 84)       \n",
    "        # convert matrix with 120 features to a matrix of 84 features (columns)\n",
    "        self.fc3 = nn.Linear(84, 10)        \n",
    "        # convert matrix with 84 features to a matrix of 10 features (columns)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        # max-pooling with 2x2 grid \n",
    "        x = self.max_pool_1(x) \n",
    "        # Conv2 + ReLU\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_2(x)\n",
    "        # Conv3 + ReLU\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 5*5*120)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # FC-3\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             492\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3           [-1, 16, 10, 10]           4,720\n",
      "         MaxPool2d-4             [-1, 16, 5, 5]               0\n",
      "            Conv2d-5            [-1, 120, 5, 5]          48,120\n",
      "            Linear-6                  [-1, 120]         360,120\n",
      "            Linear-7                   [-1, 84]          10,164\n",
      "            Linear-8                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 424,466\n",
      "Trainable params: 424,466\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 1.62\n",
      "Estimated Total Size (MB): 1.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_super = SuperLeNet5()\n",
    "if args['cuda']:\n",
    "    model_super.cuda()\n",
    "\n",
    "summary(model_super, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1 # Specifies the scaling factor of images.\n",
    "\n",
    "# Define the train and test loader\n",
    "# Here we are adding our CustomRotation function to the transformations\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['test_batch_size'], shuffle=False, **kwargs)\n",
    "\n",
    "def train(epoch):\n",
    "    model_super.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #Variables in Pytorch are differenciable. \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #This will zero out the gradients for this batch. \n",
    "        optimizer.zero_grad()\n",
    "        output = model_super(data)\n",
    "        # Calculate the loss The negative log likelihood loss. \n",
    "        # It is useful to train a classification problem with C classes.\n",
    "        loss = F.nll_loss(output, target)\n",
    "        #dloss/dx for every Variable \n",
    "        loss.backward()\n",
    "        #to do a one-step update on our parameter.\n",
    "        optimizer.step()\n",
    "        #Print out the loss periodically. \n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "def test():\n",
    "    model_super.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        with torch.no_grad(): # volatile was removed and now \n",
    "            # has no effect. Use `with torch.no_grad():` instead.\n",
    "            data= Variable(data)\n",
    "        target = Variable(target)\n",
    "        output = model_super(data)\n",
    "        # sum up batch loss # size_average and reduce args will \n",
    "        # be deprecated, please use reduction='sum' instead.\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').data \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1] \n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304572\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.313608\n",
      "\n",
      "Test set: Average loss: 0.1684, Accuracy: 9461/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.171967\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.156084\n",
      "\n",
      "Test set: Average loss: 0.1269, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.098832\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.089912\n",
      "\n",
      "Test set: Average loss: 0.0934, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.070504\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.067379\n",
      "\n",
      "Test set: Average loss: 0.0802, Accuracy: 9750/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.068391\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.082421\n",
      "\n",
      "Test set: Average loss: 0.1051, Accuracy: 9698/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.047800\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.051924\n",
      "\n",
      "Test set: Average loss: 0.0844, Accuracy: 9753/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.061874\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.056092\n",
      "\n",
      "Test set: Average loss: 0.0706, Accuracy: 9789/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.045849\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.047674\n",
      "\n",
      "Test set: Average loss: 0.1013, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.050103\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.047874\n",
      "\n",
      "Test set: Average loss: 0.0781, Accuracy: 9789/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.056856\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.029147\n",
      "\n",
      "Test set: Average loss: 0.0804, Accuracy: 9798/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.038802\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.039156\n",
      "\n",
      "Test set: Average loss: 0.0679, Accuracy: 9818/10000 (98%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.030280\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.044896\n",
      "\n",
      "Test set: Average loss: 0.0768, Accuracy: 9791/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.041343\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.039112\n",
      "\n",
      "Test set: Average loss: 0.0938, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.044047\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.041777\n",
      "\n",
      "Test set: Average loss: 0.1072, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.049226\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.022261\n",
      "\n",
      "Test set: Average loss: 0.0924, Accuracy: 9775/10000 (97%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.043483\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.059081\n",
      "\n",
      "Test set: Average loss: 0.0804, Accuracy: 9793/10000 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.042040\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.016627\n",
      "\n",
      "Test set: Average loss: 0.0747, Accuracy: 9812/10000 (98%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.018499\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.029946\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9783/10000 (97%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.038755\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.015826\n",
      "\n",
      "Test set: Average loss: 0.0865, Accuracy: 9797/10000 (97%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.032997\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 0.034328\n",
      "\n",
      "Test set: Average loss: 0.0834, Accuracy: 9797/10000 (97%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.018436\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 0.024998\n",
      "\n",
      "Test set: Average loss: 0.0827, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.030537\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 0.021434\n",
      "\n",
      "Test set: Average loss: 0.0979, Accuracy: 9782/10000 (97%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.026683\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 0.013146\n",
      "\n",
      "Test set: Average loss: 0.0833, Accuracy: 9809/10000 (98%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.024426\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 0.066471\n",
      "\n",
      "Test set: Average loss: 0.1074, Accuracy: 9760/10000 (97%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.028854\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 0.030368\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9788/10000 (97%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.026185\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 0.030596\n",
      "\n",
      "Test set: Average loss: 0.0871, Accuracy: 9802/10000 (98%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.048548\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 0.050810\n",
      "\n",
      "Test set: Average loss: 0.1035, Accuracy: 9755/10000 (97%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.037022\n",
      "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 0.052682\n",
      "\n",
      "Test set: Average loss: 0.1054, Accuracy: 9773/10000 (97%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.013192\n",
      "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 0.021835\n",
      "\n",
      "Test set: Average loss: 0.0926, Accuracy: 9808/10000 (98%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.016078\n",
      "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 0.017413\n",
      "\n",
      "Test set: Average loss: 0.1095, Accuracy: 9795/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), \n",
    "#                       lr=args['lr'], momentum=args['momentum'])\n",
    "optimizer = optim.Adam(model_super.parameters(), lr=args['lr'])\n",
    "\n",
    "# Training loop. \n",
    "# Change `args['log_interval']` if you want to change logging behavior.\n",
    "# We test the network in each epoch.\n",
    "# Setting the bool `args['train_now']` to not run training all the time.\n",
    "# We'll save the weights and use the saved weights instead of \n",
    "# training the network everytime we load the jupyter notebook.\n",
    "\n",
    "\n",
    "args['train_now'] = True\n",
    "\n",
    "if args['train_now']:\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    torch.save(model_super.state_dict(), 'models/superlenet5_normal_mnist.pytrh')\n",
    "else:\n",
    "    if args['cuda']:\n",
    "        device = torch.device(\"cuda\")\n",
    "        model_super.load_state_dict(torch.load('models/superlenet5_normal_mnist.pytrh'))\n",
    "        model_super.to(device)\n",
    "    else:\n",
    "        model_super.load_state_dict(torch.load('models/superlenet5_normal_mnist.pytrh'))\n",
    "    model_super.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the image to 29%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class:  7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f90c646e6a0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale = 0.29 # Specifies the scaling factor of images.\n",
    "\n",
    "# Define the train and test loader\n",
    "# Here we are adding our CustomRotation function to the transformations\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['test_batch_size'], shuffle=False, **kwargs)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(\"Predicted Class: \", \n",
    "      np.argmax(model_super.forward(example_data[0].unsqueeze_(0).cuda()).cpu().detach().numpy()))\n",
    "\n",
    "plt.imshow(example_data[0].cuda().cpu().detach().numpy()[0], cmap='gray')\n",
    "# transforms.functional.to_pil_image(example_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuperLeNet5(nn.Module):          \n",
    "     \n",
    "    def __init__(self):     \n",
    "        super(DuperLeNet5, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given \n",
    "        # as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, \n",
    "                                     kernel_size=3, stride=1, padding=2)\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, \n",
    "                                     kernel_size=4, stride=1, padding=2)\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, \n",
    "                                     kernel_size=3, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(10*10*120, 120)\n",
    "        # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
    "        self.fc2 = nn.Linear(120, 84)       \n",
    "        # convert matrix with 120 features to a matrix of 84 features (columns)\n",
    "        self.fc3 = nn.Linear(84, 10)        \n",
    "        # convert matrix with 84 features to a matrix of 10 features (columns)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        # max-pooling with 2x2 grid \n",
    "        x = self.max_pool_1(x) \n",
    "        # Conv2 + ReLU\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_2(x)\n",
    "        # Conv3 + ReLU\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 10*10*120)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # FC-3\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 30, 30]              60\n",
      "         MaxPool2d-2            [-1, 6, 15, 15]               0\n",
      "            Conv2d-3           [-1, 16, 16, 16]           1,552\n",
      "         MaxPool2d-4             [-1, 16, 8, 8]               0\n",
      "            Conv2d-5          [-1, 120, 10, 10]          17,400\n",
      "            Linear-6                  [-1, 120]       1,440,120\n",
      "            Linear-7                   [-1, 84]          10,164\n",
      "            Linear-8                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 1,470,146\n",
      "Trainable params: 1,470,146\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.18\n",
      "Params size (MB): 5.61\n",
      "Estimated Total Size (MB): 5.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_duper = DuperLeNet5()\n",
    "if args['cuda']:\n",
    "    model_duper.cuda()\n",
    "\n",
    "summary(model_duper, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.024726\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.038723\n",
      "\n",
      "Test set: Average loss: 0.0478, Accuracy: 9854/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.015334\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.024020\n",
      "\n",
      "Test set: Average loss: 0.0389, Accuracy: 9888/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.015300\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.017980\n",
      "\n",
      "Test set: Average loss: 0.0435, Accuracy: 9883/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.010114\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.009879\n",
      "\n",
      "Test set: Average loss: 0.0419, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.011848\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.011133\n",
      "\n",
      "Test set: Average loss: 0.0415, Accuracy: 9889/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004958\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.014925\n",
      "\n",
      "Test set: Average loss: 0.0500, Accuracy: 9887/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.008834\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.003011\n",
      "\n",
      "Test set: Average loss: 0.0547, Accuracy: 9870/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.025902\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.006423\n",
      "\n",
      "Test set: Average loss: 0.0404, Accuracy: 9895/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.005032\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.007753\n",
      "\n",
      "Test set: Average loss: 0.0480, Accuracy: 9890/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.014041\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.013036\n",
      "\n",
      "Test set: Average loss: 0.0477, Accuracy: 9893/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scale = 1 # Specifies the scaling factor of images.\n",
    "\n",
    "# Define the train and test loader\n",
    "# Here we are adding our CustomRotation function to the transformations\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['test_batch_size'], shuffle=False, **kwargs)\n",
    "\n",
    "args['epochs']=10\n",
    "\n",
    "def train(epoch):\n",
    "    model_duper.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #Variables in Pytorch are differenciable. \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #This will zero out the gradients for this batch. \n",
    "        optimizer.zero_grad()\n",
    "        output = model_duper(data)\n",
    "        # Calculate the loss The negative log likelihood loss. \n",
    "        # It is useful to train a classification problem with C classes.\n",
    "        loss = F.nll_loss(output, target)\n",
    "        #dloss/dx for every Variable \n",
    "        loss.backward()\n",
    "        #to do a one-step update on our parameter.\n",
    "        optimizer.step()\n",
    "        #Print out the loss periodically. \n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "\n",
    "def test():\n",
    "    model_duper.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        with torch.no_grad(): # volatile was removed and now \n",
    "            # has no effect. Use `with torch.no_grad():` instead.\n",
    "            data= Variable(data)\n",
    "        target = Variable(target)\n",
    "        output = model_duper(data)\n",
    "        # sum up batch loss # size_average and reduce args will \n",
    "        # be deprecated, please use reduction='sum' instead.\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').data \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1] \n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "# optimizer = optim.SGD(model.parameters(), \n",
    "#                       lr=args['lr'], momentum=args['momentum'])\n",
    "optimizer = optim.Adam(model_duper.parameters(), lr=args['lr'])\n",
    "\n",
    "# Training loop. \n",
    "# Change `args['log_interval']` if you want to change logging behavior.\n",
    "# We test the network in each epoch.\n",
    "# Setting the bool `args['train_now']` to not run training all the time.\n",
    "# We'll save the weights and use the saved weights instead of \n",
    "# training the network everytime we load the jupyter notebook.\n",
    "\n",
    "\n",
    "args['train_now'] = True\n",
    "\n",
    "if args['train_now']:\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    torch.save(model_duper.state_dict(), 'models/duperlenet5_normal_mnist.pytrh')\n",
    "else:\n",
    "    if args['cuda']:\n",
    "        device = torch.device(\"cuda\")\n",
    "        model_duper.load_state_dict(torch.load('models/duperlenet5_normal_mnist.pytrh'))\n",
    "        model_duper.to(device)\n",
    "    else:\n",
    "        model_duper.load_state_dict(torch.load('models/duperlenet5_normal_mnist.pytrh'))\n",
    "    model_duper.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f90c6651a20>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale = 0.2 # Specifies the scaling factor of images.\n",
    "\n",
    "# Define the train and test loader\n",
    "# Here we are adding our CustomRotation function to the transformations\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       CustomScaling(scale),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['test_batch_size'], shuffle=False, **kwargs)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(\"Predicted Class: \", \n",
    "      np.argmax(model_duper.forward(example_data[0].unsqueeze_(0).cuda()).cpu().detach().numpy()))\n",
    "\n",
    "plt.imshow(example_data[0].cuda().cpu().detach().numpy()[0], cmap='gray')\n",
    "# transforms.functional.to_pil_image(example_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
